{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kldjFegYEqMq",
        "outputId": "e687f714-1030-4936-9dfc-07e0a5ff3083"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/research/feedback_controller/data.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWW6iYPoF2zC",
        "outputId": "cd3a2159-c9ce-42a2-e729-2957bb2fec07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/research/feedback_controller/data.zip\n",
            "   creating: data/\n",
            "  inflating: data/torobo_eyes_view_1.jpeg  \n",
            "  inflating: data/torobo_eyes_view_2.jpeg  \n",
            "  inflating: data/torobo_eyes_view_3.jpeg  \n",
            "  inflating: data/torobo_eyes_view_4.jpeg  \n",
            "  inflating: data/torobo_eyes_view_5.jpeg  \n",
            "  inflating: data/torobo_eyes_view_6.jpeg  \n",
            "  inflating: data/torobo_eyes_view_7.jpeg  \n",
            "  inflating: data/torobo_eyes_view_8.jpeg  \n",
            "  inflating: data/torobo_eyes_view_9.jpeg  \n",
            "  inflating: data/torobo_eyes_view_10.jpeg  \n",
            "  inflating: data/torobo_eyes_view_11.jpeg  \n",
            "  inflating: data/torobo_eyes_view_12.jpeg  \n",
            "  inflating: data/torobo_eyes_view_13.jpeg  \n",
            "  inflating: data/torobo_eyes_view_14.jpeg  \n",
            "  inflating: data/torobo_eyes_view_15.jpeg  \n",
            "  inflating: data/torobo_eyes_view_16.jpeg  \n",
            "  inflating: data/torobo_eyes_view_17.jpeg  \n",
            "  inflating: data/torobo_eyes_view_18.jpeg  \n",
            "  inflating: data/torobo_eyes_view_19.jpeg  \n",
            "  inflating: data/torobo_eyes_view_20.jpeg  \n",
            "  inflating: data/torobo_eyes_view_21.jpeg  \n",
            "  inflating: data/torobo_eyes_view_22.jpeg  \n",
            "  inflating: data/torobo_eyes_view_23.jpeg  \n",
            "  inflating: data/torobo_eyes_view_24.jpeg  \n",
            "  inflating: data/torobo_eyes_view_25.jpeg  \n",
            "  inflating: data/torobo_eyes_view_26.jpeg  \n",
            "  inflating: data/torobo_eyes_view_27.jpeg  \n",
            "  inflating: data/torobo_eyes_view_28.jpeg  \n",
            "  inflating: data/torobo_eyes_view_29.jpeg  \n",
            "  inflating: data/torobo_eyes_view_30.jpeg  \n",
            "  inflating: data/torobo_eyes_view_31.jpeg  \n",
            "  inflating: data/torobo_eyes_view_32.jpeg  \n",
            "  inflating: data/torobo_eyes_view_33.jpeg  \n",
            "  inflating: data/torobo_eyes_view_34.jpeg  \n",
            "  inflating: data/torobo_eyes_view_35.jpeg  \n",
            "  inflating: data/torobo_eyes_view_36.jpeg  \n",
            "  inflating: data/torobo_eyes_view_37.jpeg  \n",
            "  inflating: data/torobo_eyes_view_38.jpeg  \n",
            "  inflating: data/torobo_eyes_view_39.jpeg  \n",
            "  inflating: data/torobo_eyes_view_40.jpeg  \n",
            "  inflating: data/torobo_eyes_view_41.jpeg  \n",
            "  inflating: data/torobo_eyes_view_42.jpeg  \n",
            "  inflating: data/torobo_eyes_view_43.jpeg  \n",
            "  inflating: data/torobo_eyes_view_44.jpeg  \n",
            "  inflating: data/torobo_eyes_view_45.jpeg  \n",
            "  inflating: data/torobo_eyes_view_46.jpeg  \n",
            "  inflating: data/torobo_eyes_view_47.jpeg  \n",
            "  inflating: data/torobo_eyes_view_48.jpeg  \n",
            "  inflating: data/torobo_eyes_view_49.jpeg  \n",
            "  inflating: data/torobo_eyes_view_50.jpeg  \n",
            "  inflating: data/torobo_eyes_view_51.jpeg  \n",
            "  inflating: data/torobo_eyes_view_52.jpeg  \n",
            "  inflating: data/torobo_eyes_view_53.jpeg  \n",
            "  inflating: data/torobo_eyes_view_54.jpeg  \n",
            "  inflating: data/torobo_eyes_view_55.jpeg  \n",
            "  inflating: data/torobo_eyes_view_56.jpeg  \n",
            "  inflating: data/torobo_eyes_view_57.jpeg  \n",
            "  inflating: data/torobo_eyes_view_58.jpeg  \n",
            "  inflating: data/torobo_eyes_view_59.jpeg  \n",
            "  inflating: data/torobo_eyes_view_60.jpeg  \n",
            "  inflating: data/torobo_eyes_view_61.jpeg  \n",
            "  inflating: data/torobo_eyes_view_62.jpeg  \n",
            "  inflating: data/torobo_eyes_view_63.jpeg  \n",
            "  inflating: data/torobo_eyes_view_64.jpeg  \n",
            "  inflating: data/torobo_eyes_view_65.jpeg  \n",
            "  inflating: data/torobo_eyes_view_66.jpeg  \n",
            "  inflating: data/torobo_eyes_view_67.jpeg  \n",
            "  inflating: data/torobo_eyes_view_68.jpeg  \n",
            "  inflating: data/torobo_eyes_view_69.jpeg  \n",
            "  inflating: data/torobo_eyes_view_70.jpeg  \n",
            "  inflating: data/trajectory.npy     \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/drive/MyDrive/research/feedback_controller/weights"
      ],
      "metadata": {
        "id": "L5QbBJyzdFxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kR3Ev0Px7Qf_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AlexNetPT(nn.Module):\n",
        "    def __init__(self, encoded_space_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        # Load the pre-trained AlexNet model\n",
        "        alexnet = models.alexnet(pretrained=True)\n",
        "\n",
        "        # Get the feature extraction layers\n",
        "        self.feature_extractor = alexnet.features\n",
        "\n",
        "        self.flatten = nn.Flatten(start_dim=1)\n",
        "\n",
        "        self.encoder_lin = nn.Sequential(\n",
        "            nn.Linear(256*6*6, 1024),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(1024, encoded_space_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.feature_extractor(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.encoder_lin(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, encoded_space_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(encoded_space_dim, 1024),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(1024, 1024),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(1024, encoded_space_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class GeneralModel(nn.Module):\n",
        "    def __init__(self, encoded_space_dim):\n",
        "        super().__init__()\n",
        "        self.alexnet = AlexNetPT(encoded_space_dim)\n",
        "        self.mlp = MLP(encoded_space_dim)\n",
        "\n",
        "    def forward(self, img_tensor, joints):\n",
        "        x_des = self.alexnet(img_tensor)\n",
        "        x_des = torch.squeeze(x_des, 0)\n",
        "        mlp_inp = x_des - joints\n",
        "        joint_pred = self.mlp(mlp_inp.float())\n",
        "\n",
        "        return joint_pred"
      ],
      "metadata": {
        "id": "NtSHyf7c7ZWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"device:\", device)\n",
        "\n",
        "model = GeneralModel(encoded_space_dim=7)\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCy37zUeH1PR",
        "outputId": "8fbfc2d4-9278-4295-87fc-783400045056"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n",
            "100%|██████████| 233M/233M [00:00<00:00, 258MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12.980046 M parameters\n",
            "GeneralModel(\n",
            "  (alexnet): AlexNetPT(\n",
            "    (feature_extractor): Sequential(\n",
            "      (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
            "      (1): ReLU(inplace=True)\n",
            "      (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "      (4): ReLU(inplace=True)\n",
            "      (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (7): ReLU(inplace=True)\n",
            "      (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (9): ReLU(inplace=True)\n",
            "      (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (11): ReLU(inplace=True)\n",
            "      (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "    (encoder_lin): Sequential(\n",
            "      (0): Linear(in_features=9216, out_features=1024, bias=True)\n",
            "      (1): ReLU(inplace=True)\n",
            "      (2): Linear(in_features=1024, out_features=7, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (mlp): MLP(\n",
            "    (linear): Sequential(\n",
            "      (0): Linear(in_features=7, out_features=1024, bias=True)\n",
            "      (1): ReLU(inplace=True)\n",
            "      (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (3): ReLU(inplace=True)\n",
            "      (4): Linear(in_features=1024, out_features=7, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 1000\n",
        "learning_rate = 3e-4"
      ],
      "metadata": {
        "id": "Oz21dE2GDvGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_indices = list(range(1, 71))\n",
        "random.shuffle(all_indices)\n",
        "\n",
        "train_indices = all_indices[:65]\n",
        "test_indices = all_indices[65:]\n",
        "print(train_indices)\n",
        "print(test_indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PTSzSwUD-dP",
        "outputId": "9412843d-754f-45c4-b73d-3bb200cd690e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[69, 23, 29, 55, 56, 3, 18, 66, 38, 51, 24, 25, 36, 21, 54, 26, 9, 1, 43, 5, 4, 44, 30, 10, 45, 50, 16, 59, 8, 39, 46, 7, 19, 70, 42, 15, 41, 33, 34, 22, 13, 2, 62, 58, 27, 40, 60, 12, 49, 68, 32, 65, 48, 6, 28, 61, 37, 11, 64, 67, 31, 14, 52, 35, 20]\n",
            "[17, 57, 47, 53, 63]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "                transforms.Resize([224, 224]), # Resizing the image as the VGG only take 224 x 244 as input size\n",
        "                # transforms.RandomHorizontalFlip(), # Flip the data horizontally\n",
        "                transforms.ToTensor(),\n",
        "                # transforms.Normalize(mean=(0.5,0.5,0.5), std=(0.5,0.5,0.5))\n",
        "            ])"
      ],
      "metadata": {
        "id": "GGZefzVrEXY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trajectories = np.load(\"./data/trajectory.npy\")\n",
        "trajectories.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DoIlEFOSILpY",
        "outputId": "9b2737b9-478e-4569-f86f-5725372c4969"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(70, 50, 7)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "7VDNxdtiHiAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_test():\n",
        "    random.shuffle(test_indices)\n",
        "    for idx in test_indices:\n",
        "        trajectory = trajectories[idx-1, :, :]\n",
        "        img = Image.open(f\"./data/torobo_eyes_view_{idx}.jpeg\")\n",
        "        img_tensor = transform(img)\n",
        "        img_tensor = torch.unsqueeze(img_tensor, 0)\n",
        "        img_tensor = img_tensor.to(device)\n",
        "\n",
        "        model.eval()\n",
        "        traj_loss = 0\n",
        "        randint = random.randint(0, trajectory.shape[0]-2)\n",
        "        for k in range(trajectory.shape[0]-1):\n",
        "            joints = torch.tensor(trajectory[k, :], dtype=torch.float, device=device)\n",
        "            next_joints = torch.tensor(trajectory[k+1, :], dtype=torch.float, device=device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                joints_pred = model(img_tensor, joints)\n",
        "                loss = criterion(joints_pred, next_joints)\n",
        "                traj_loss += loss.item()\n",
        "\n",
        "                if k == randint:\n",
        "                    print(\"joints_pred\", joints_pred)\n",
        "                    print(\"next_joints\", next_joints)\n",
        "\n",
        "        model.train()\n",
        "        print(f\"for index: {idx}, loss: {traj_loss}\")\n"
      ],
      "metadata": {
        "id": "AKtvRVowMwFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(epochs):\n",
        "    random.shuffle(train_indices)\n",
        "    model.train()\n",
        "    for idx in train_indices:\n",
        "        trajectory = trajectories[idx-1, :, :]\n",
        "        img = Image.open(f\"./data/torobo_eyes_view_{idx}.jpeg\")\n",
        "        img_tensor = transform(img)\n",
        "        img_tensor = torch.unsqueeze(img_tensor, 0)\n",
        "        img_tensor = img_tensor.to(device)\n",
        "\n",
        "        for k in range(trajectory.shape[0]-1):\n",
        "            joints = torch.tensor(trajectory[k, :], dtype=torch.float, device=device)\n",
        "            next_joints = torch.tensor(trajectory[k+1, :], dtype=torch.float, device=device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            joints_pred = model(img_tensor, joints)\n",
        "            loss = criterion(joints_pred, next_joints)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    if i%100 == 0:\n",
        "        print(f\"finished epoch {i}\")\n",
        "        run_test()\n",
        "        print(\"==========\")\n",
        "        torch.save(model.state_dict(), f\"/content/drive/MyDrive/research/feedback_controller/weights/fbc_{i}.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y91rcRE3D4I-",
        "outputId": "df34dfd5-6951-4e3a-c894-fd66e3d66188"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "finished epoch 0\n",
            "joints_pred tensor([ 1.5917,  0.7077, -0.2234,  1.3307,  1.7102, -0.4964,  0.9751],\n",
            "       device='cuda:0')\n",
            "next_joints tensor([ 1.5863,  0.6760, -0.1626,  1.4063,  1.6808, -0.5013,  1.0493],\n",
            "       device='cuda:0')\n",
            "for index: 57, loss: 0.41068867268040776\n",
            "joints_pred tensor([ 1.2300,  0.6766,  0.2012,  1.1387,  1.3206, -0.3674,  1.0601],\n",
            "       device='cuda:0')\n",
            "next_joints tensor([ 1.1846,  0.7332,  0.4262,  1.2966,  1.2734, -0.3981,  1.2745],\n",
            "       device='cuda:0')\n",
            "for index: 17, loss: 1.3057027403265238\n",
            "joints_pred tensor([ 1.6924,  0.7404, -0.2913,  1.4037,  1.8099, -0.5100,  0.9859],\n",
            "       device='cuda:0')\n",
            "next_joints tensor([ 1.6706,  0.4542, -0.3057,  1.5329,  1.7138, -0.3937,  0.9831],\n",
            "       device='cuda:0')\n",
            "for index: 63, loss: 0.328197137627285\n",
            "joints_pred tensor([ 1.5581,  0.6916, -0.1963,  1.3096,  1.6694, -0.4800,  0.9672],\n",
            "       device='cuda:0')\n",
            "next_joints tensor([ 1.5433,  0.5841, -0.1140,  1.3779,  1.6318, -0.3848,  1.0700],\n",
            "       device='cuda:0')\n",
            "for index: 53, loss: 0.5002749818377197\n",
            "joints_pred tensor([ 1.5141,  0.6883, -0.1670,  1.2918,  1.6206, -0.4612,  0.9588],\n",
            "       device='cuda:0')\n",
            "next_joints tensor([ 1.4935,  0.4795, -0.0770,  1.5376,  1.6092, -0.4449,  1.0538],\n",
            "       device='cuda:0')\n",
            "for index: 47, loss: 0.8370958445593715\n",
            "==========\n",
            "finished epoch 100\n",
            "joints_pred tensor([ 1.1557,  0.7294,  0.4669,  1.3002,  1.2577, -0.4111,  1.2738],\n",
            "       device='cuda:0')\n",
            "next_joints tensor([ 1.1915,  0.7477,  0.4241,  1.2834,  1.2692, -0.3991,  1.2780],\n",
            "       device='cuda:0')\n",
            "for index: 17, loss: 0.020050556282512844\n",
            "joints_pred tensor([ 1.5610,  0.6652, -0.0887,  1.3099,  1.6324, -0.3933,  1.0729],\n",
            "       device='cuda:0')\n",
            "next_joints tensor([ 1.5466,  0.6764, -0.0985,  1.2779,  1.6304, -0.3772,  1.0925],\n",
            "       device='cuda:0')\n",
            "for index: 53, loss: 0.013508027346688323\n",
            "joints_pred tensor([ 1.6025,  0.4467, -0.2108,  1.5604,  1.6750, -0.4107,  1.0033],\n",
            "       device='cuda:0')\n",
            "next_joints tensor([ 1.5934,  0.4463, -0.2081,  1.5316,  1.6669, -0.3949,  1.0141],\n",
            "       device='cuda:0')\n",
            "for index: 57, loss: 0.008177645162504632\n",
            "joints_pred tensor([ 1.6788,  0.5202, -0.2925,  1.5295,  1.7328, -0.4373,  0.9859],\n",
            "       device='cuda:0')\n",
            "next_joints tensor([ 1.6629,  0.5376, -0.2848,  1.4727,  1.7217, -0.4170,  0.9985],\n",
            "       device='cuda:0')\n",
            "for index: 63, loss: 0.01921918644802645\n",
            "joints_pred tensor([ 1.4936,  0.4380, -0.0732,  1.5584,  1.6049, -0.4171,  1.0480],\n",
            "       device='cuda:0')\n",
            "next_joints tensor([ 1.4935,  0.4672, -0.0781,  1.5370,  1.6088, -0.4319,  1.0530],\n",
            "       device='cuda:0')\n",
            "for index: 47, loss: 0.004097202752745943\n",
            "==========\n",
            "finished epoch 200\n",
            "joints_pred tensor([ 1.4915,  0.4560, -0.0806,  1.5442,  1.6073, -0.4140,  1.0626],\n",
            "       device='cuda:0')\n",
            "next_joints tensor([ 1.4933,  0.4382, -0.0807,  1.5356,  1.6077, -0.4015,  1.0513],\n",
            "       device='cuda:0')\n",
            "for index: 47, loss: 0.00321697323488479\n",
            "joints_pred tensor([ 1.6672,  0.4728, -0.3072,  1.5502,  1.7228, -0.4131,  0.9941],\n",
            "       device='cuda:0')\n",
            "next_joints tensor([ 1.6706,  0.4542, -0.3057,  1.5328,  1.7138, -0.3937,  0.9831],\n",
            "       device='cuda:0')\n",
            "for index: 63, loss: 0.009672928834334016\n",
            "joints_pred tensor([ 1.5966,  0.5166, -0.2041,  1.5143,  1.6738, -0.4380,  1.0240],\n",
            "       device='cuda:0')\n",
            "next_joints tensor([ 1.5919,  0.4971, -0.1981,  1.5039,  1.6700, -0.4184,  1.0219],\n",
            "       device='cuda:0')\n",
            "for index: 57, loss: 0.0057447112194495276\n",
            "joints_pred tensor([ 1.5401,  0.4586, -0.1419,  1.5404,  1.6378, -0.4106,  1.0441],\n",
            "       device='cuda:0')\n",
            "next_joints tensor([ 1.5381,  0.4419, -0.1379,  1.5319,  1.6340, -0.3965,  1.0352],\n",
            "       device='cuda:0')\n",
            "for index: 53, loss: 0.008804441422398668\n",
            "joints_pred tensor([ 1.0719,  0.4970,  0.4514,  1.5107,  1.3384, -0.3918,  1.2227],\n",
            "       device='cuda:0')\n",
            "next_joints tensor([ 1.0704,  0.4905,  0.4608,  1.5162,  1.3432, -0.3815,  1.2176],\n",
            "       device='cuda:0')\n",
            "for index: 17, loss: 0.0021601372418444953\n",
            "==========\n",
            "finished epoch 300\n",
            "joints_pred tensor([ 1.1212,  0.6270,  0.4364,  1.4380,  1.2786, -0.4146,  1.2494],\n",
            "       device='cuda:0')\n",
            "next_joints tensor([ 1.1377,  0.6335,  0.4404,  1.3868,  1.3021, -0.3913,  1.2512],\n",
            "       device='cuda:0')\n",
            "for index: 17, loss: 0.02419314438884612\n",
            "joints_pred tensor([ 1.6574,  0.7053, -0.2401,  1.4043,  1.7442, -0.4805,  1.0140],\n",
            "       device='cuda:0')\n",
            "next_joints tensor([ 1.6477,  0.7010, -0.2437,  1.3550,  1.7372, -0.4628,  1.0287],\n",
            "       device='cuda:0')\n",
            "for index: 63, loss: 0.02112579662934877\n",
            "joints_pred tensor([ 1.5941,  0.4551, -0.2024,  1.5670,  1.6587, -0.4143,  1.0146],\n",
            "       device='cuda:0')\n",
            "next_joints tensor([ 1.5934,  0.4463, -0.2081,  1.5316,  1.6669, -0.3948,  1.0141],\n",
            "       device='cuda:0')\n",
            "for index: 57, loss: 0.014792481902986765\n",
            "joints_pred tensor([ 1.5278,  0.4485, -0.1225,  1.5648,  1.6226, -0.4129,  1.0373],\n",
            "       device='cuda:0')\n",
            "next_joints tensor([ 1.5381,  0.4420, -0.1379,  1.5318,  1.6340, -0.3964,  1.0352],\n",
            "       device='cuda:0')\n",
            "for index: 53, loss: 0.019972106820205227\n",
            "joints_pred tensor([ 1.4817,  0.6438, -0.0565,  1.5718,  1.6025, -0.6285,  1.0542],\n",
            "       device='cuda:0')\n",
            "next_joints tensor([ 1.4944,  0.6424, -0.0621,  1.5455,  1.6153, -0.6154,  1.0636],\n",
            "       device='cuda:0')\n",
            "for index: 47, loss: 0.009560206235619262\n",
            "==========\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H4JhHBZ8Iy_o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}